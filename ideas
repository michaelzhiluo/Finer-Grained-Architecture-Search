Idea #1: learning the number of steps in the inner loop of gradient descent
    - can use reparameterization gradients to treat the number of steps as a latent variable!
    - what is the appropriate distribution to use as a model here? Poisson?
    - need to read the literature on reparameterization gradients:
        - The Generalized Reparameterization Gradient (NIPS 2016) https://arxiv.org/abs/1610.02287
        - Pathwise Derivatives Beyond the Reparameterization Trick (ICML 2018) https://arxiv.org/abs/1806.01851
        - Implicit Reparameterization Gradients (NIPS 2018) https://arxiv.org/abs/1805.08498
        - Latent Alignment and Variational Attention (NIPS 2018) https://arxiv.org/abs/1807.03756
    - can be incorporated into a larger procedure via stochastic variational EM
        - Variational algorithms for approximate Bayesian inference https://cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf
    - will this allow us to avoid the short-horizon bias? (https://arxiv.org/abs/1803.02021)

Idea #2: gradient-based meta-learning for optimization for learning neural network architecture
    - DARTS is too restrictive -- only searches over a small set of candidate nodes
    - can we parameterize invariances? (e.g., the translational invariance encoded into a convolutional network)
