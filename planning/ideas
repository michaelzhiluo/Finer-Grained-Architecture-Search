Idea #1: learning the number of steps in the inner loop of gradient descent
    - can use reparameterization gradients to treat the number of steps as a latent variable!
    - what is the appropriate distribution to use as a model here? Poisson?
    - need to read the literature on reparameterization gradients:
        - The Generalized Reparameterization Gradient (NIPS 2016) https://arxiv.org/abs/1610.02287
        - Pathwise Derivatives Beyond the Reparameterization Trick (ICML 2018) https://arxiv.org/abs/1806.01851
        - Implicit Reparameterization Gradients (NIPS 2018) https://arxiv.org/abs/1805.08498
        - Latent Alignment and Variational Attention (NIPS 2018) https://arxiv.org/abs/1807.03756
    - can be incorporated into a larger procedure via stochastic variational EM
        - Variational algorithms for approximate Bayesian inference https://cse.buffalo.edu/faculty/mbeal/papers/beal03.pdf
    - will this allow us to avoid the short-horizon bias? (https://arxiv.org/abs/1803.02021)

Idea #2: gradient-based meta-learning for optimization for learning neural network architecture
    - DARTS is too restrictive -- only searches over a small set of candidate nodes
    - can we parameterize invariances? (e.g., the translational invariance encoded into a convolutional network)
    -What is the gradient of invariances between convolution to fully connected
    
2a)
    -Generalized Convolutions (with no invariances, but invariances are a subset of it):
        -we have a set of N weights for each layer
        -Input is Ix1, output is Ox1
        -Define a connection as a triplet (I_i, W_j, O_k)
            -At the end, sum all the connections
        -MAximum of I*N*O conections 
            -how to we learn those connections? And what type of connections parameterize invariance?
    
Idea #3: Ways to do discrete and continuous hyperparmaeter optimization
    -Concrete Distribution for discrete hyperparemeters
    -Optimization of all hyparameters -> neural network architecture search is solved
