7 papers a week

Differentiable Neural Network Architecture Search
https://openreview.net/pdf?id=BJ-MRKkwG

Simplifying Neural Networks by Soft Weight-sharing
http://www.cs.toronto.edu/~hinton/absps/sunspots.pdf

Learning Structured Sparsity in Deep Neural Networks
https://arxiv.org/abs/1608.03665

Soft Weight Sharing for NN compression
https://arxiv.org/pdf/1702.04008.pdf
    -Use the soft weight-sharing idea of Nowlan & Hinton (1992) for NN compression.
    -Motivation: Learn a mixture of Gaussians (MoG) prior to encode parameters.
    -Idea: "Compress" weights to K clusters. Works if weights concentrate closely around cluster means.
    -Algorithm: 
        -Take a pretrained network. Initialize a MoG w/ diagonal covariances "prior" over the weights.
        -Retrain model with joint log-likelihood (Eq. 7).
        -(Optional: Merge MoG components as necessary using a KL. Easy because component likelihoods are Gaussian!)
        -Quantize by setting each weight to the mean of the component with highest responsibility.
    -Results:
        -Decent for LeNet on MNIST. No baseline for (wide) ResNet.
        -Authors state difficulty with VGG:
            -"In experiences with VGG we were able to prune 93% of the 
             weights without loss of accuracy, however, the quantization step resulted in significant loss of 
             accuracy. We think this is due to the network not having convergened."
    

Meta Networks
https://arxiv.org/pdf/1703.00837.pdf

MAML
https://arxiv.org/pdf/1703.03400.pdf

Meta-learning Shared Hierarchies
https://openreview.net/pdf?id=SyX0IeWAW

Meta-Learning Evolutionary Artificial Neural Networks
https://arxiv.org/ftp/cs/papers/0405/0405024.pdf

Weight Uncertainty in Neural Networks
https://arxiv.org/pdf/1505.05424.pdf

Gradient-based Hyperparameter Optimization through Reversible Learning
https://arxiv.org/abs/1502.03492
    -Most papers that perform gradient based hyperparmaeter optimization stores all the weights w1, ... w_t to find
    gradient for hyperparameters at time step 1
    -This paper finds a way via momentum descent to find gradients w.r.t hyperparemters in O(T) time with constant memory proportional
    to number of weights, not W*T. 
    -Lots of cool applications in the paper (you can treat initialized weights as a hyperparameter! (used later in MAML))
    -CON: to improve hyperparameters, you gotta do a bit of training than go from time T to time 1 to obtain gradient for hyperparameters 
    at that timestep
    -Weightsharing? Between interations of meta improvements?

Hyperparameter optimization with approximate gradient
https://arxiv.org/abs/1602.02355
    -Pretty sure DART's main inspiration was off this paper, the algorithms are very very similar
    -Same idea where you hyperparameter and parameter updating are done in a bi-level optimization like in DART


https://arxiv.org/abs/1608.06993
https://arxiv.org/abs/1711.05611

Winning Ticket Hypothesis
https://arxiv.org/abs/1803.03635
    - Algorithm:
        -Initialize Network with weights, w0
        -Loop:
            -Train till convergence
            -Prune smallest weights
            -Take the pruned network and reinitialize the remaining weights to w0
    - Larger networks have a larger combination of "winning tickets" (subnetworks that have a good combination during 
    initialization w0 that result in high accuracy), hence the idea of pruning
    -When randomly reinitialized or rearranged, winning tickets perform far worse than the original network, meaning neither structure nor
    initialization alone is responsible for a winning ticketâ€™s success.
    - how do they parameterize the pruned network? do they zero out the weights?

DART Architecture Search
https://arxiv.org/abs/1806.09055
    -Optimizes based on cell, not entire network (convolutional or recurrent cell like LSTM or GRU)
    -How Their Cell model works
        -Imagine the cell as a dag with nodes
            -2 nodes for input (from previous 2 convolutional layers) and 1 node for output
            -Cell also contains n intermediary nodes, x1, x2, x3, .... xn
            -The edge connecting between two nodes is a mathematical operation (conv, zero, etc)
                -M mathematical operations (could be anything wtih trainable weights)
            -There are LOTS of edges
                -Both input nodes are connected to x1, ... xn
                -All intermediary nodes are connected to output node
                -For all i<j, node x_i connects to x_j
                -Between any two nodes with a valid connection, there are M edges, each of which is a mathematical operation
                    -IMPORTANT: The output of the M edges is a weighted softmaxed sum of all M mathematical operations
                        -Adds another set of weights, alpha_(i,j), to represent how much each of those M mathemtical operations 
                        should be weighted
    -This becomes a cool dual optimization problem:
        -Given a fixed alpha_(i,j), optimize weights over training set
        -Given fixed weights, optimize alpha over validation set
            -Seems like, since this is no longer the setting of few-shot learning, we should just call this batches 1 & 2!
    -The above optimization problem is expensive, hence the DART algorithm:
        -For each interation of training over weights w, update weights over training set, then update alpha over validation set
            -Different than finding optimal w*, then updating alpha to alpha', then finding optimal w**, etc.
        -Another caveat is that they choose the largest alpha as the only connection between nodes after training
            -ie., this is how they go from the relaxed problem to the original setting of a single architecture
            -Q: is this the best way to relax the architecture search problem?


Neural Module Networks
https://arxiv.org/pdf/1511.02799.pdf
-Not really weight sharing going on here, unless we interpret modularization as a form of weight sharing

Stackable NN Modules
https://arxiv.org/abs/1807.08556
-Don't see the weight sharing in this example
