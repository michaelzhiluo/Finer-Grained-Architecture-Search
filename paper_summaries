

Spatial Transformer Networks (RIP invariance idea)
https://arxiv.org/pdf/1506.02025.pdf
	-Aight invariance rip
	-New module that learns an affine transofmration (six degress of freedom) and applies it on the input
	-Add module to CNN to learn affine invariance
	
Permutation-equivariant neural networks applied to dynamics prediction
https://arxiv.org/pdf/1612.04530.pdf
	-Utilizes idea that permutation invariance is via addition

Scale Invariant Convolutional Neural Network
https://arxiv.org/abs/1411.6369
	-Similar to the techniques used in most invariance papers: Add more kernels!
	-Basically, you train one conv kernel for a specific scale and you add more kernels that you scale up or scale down to perform additional convs
	-Concatenate all the feature vectors from the conv kernels of all scales

Locally Scale-Invariant Convolutional Neural Networks
	https://people.eecs.berkeley.edu/~kanazawa/papers/sicnn_workshop2014.pdf
	-Form laplacian pyramid of scaled images form small to big, convovle the same kenrel around them
	-Maxpool

Rotation Invariance Neural Network
https://arxiv.org/ftp/arxiv/papers/1706/1706.05534.pdf
	-You have a 3D feature map
	-Bascially the kernel is the size of feature map in that channel
	-In addition, when you convolve, you dont pad with 0 but have the kernel wrap around 
	-YOU ADD A LOT OF KERNELS PER LAYER, duplicating each kernel n times for the n possible rotation sof image
	-This paper kind of copies the "Efficient and Invariant CNN for Dnese Prediction", moving on

Quantifying Translation-Invariance in Convolutional Neural Networks
http://cs231n.stanford.edu/reports/2016/pdfs/107_Report.pdf
	-Deeper neural networks and larger filter sizes are more translationally invariant
	-MOST OF ALL, training datset matters most (include translations in your dataset!)

Efficient and Invariant Convolutional Neural Networks for Dense Prediction
https://arxiv.org/pdf/1711.09064.pdf
	-Rotate your kernel for rotational invariance, Flip your kernel for flip invariance
	-Added all these different rotated/flipped kernels in yoru conv layer, combine all outputs of the kernels
	by doing a maxout from Goodfellow's paper 


THE CONCRETE DISTRIBUTION: A CONTINUOUS RELAXATION OF DISCRETE RANDOM VARIABLES
https://arxiv.org/pdf/1611.00712.pdf
    -Discrete distribution defined as Discrete(alpha1, alpha2, ... alphan) aka unnormalized probabilities
    over n discrete values
    	-Probabilites of selection for each of the n discrete values would be alpha_i/(sum of all alpha)
	-Another way to look at this is to calculate
		alpha_i + G_i, where G_i is sampled from a simple Gumbel distribution, and choose the maximum value, 
		which will be your sampled value
     -Concrete Random Variables extend this alternative view by generalizing taking the max(alpha_i + G_i) by adding
     a LAMBDA, or the temperature variable, and performing a softmax over alpha_i + G_i. When lambda =0, you will get the discrete distribution
     -Concrete Distribution has a distirubtion parameterized on alpha and lambda
     -Properties of Concrete Distribution
     	-You can reparemterize distirubtion (in stochastic nodes for back prop for example) with Gumbel
	-You can round a concrete distribution to a discrete distirubtion by doing integration
	-As lmabda ->0, the pdf is discrete
	-Somwhat convex
     -Any loss function that has E_some discrete distribution(something) can be relaxed into the continuous concrete distribution
     
Learning Structured Sparsity in Deep Neural Networks
https://arxiv.org/abs/1608.03665
	-Best paper I read yet; Simple yet powerful idea
	-When you add regularization to NN, like L2, L1, some weights go to zero as we all learned in our intro ML classes
	-Conv layer dimensions have four parameters: N (number of filters), C (channel depth), W, H 
	-You can eliminate entire groups of weights if you do this:
		- Group 1,2, ... G (G groups of weights)
		-  Add this to your loss function:
			-lambda_G * sum(1 to G){ || ith Group || )
			- || ith group || is usually L2 norm for all the weights in the group
	-Implications:
		-Can eliminate entire layers if you set each group to be the weights of a layer
		-Can obtain conv kernels of any shape if you, for a single layer, group together C, W, H for all N filters of that layer
		-Most of all leave out important filters and channels if you group by channels and filters
             
Variational Dropout Sparsifies Deep Neural Networks
https://arxiv.org/abs/1701.05369
	-Dropout is typically a bernoulli variable * weights, but dropout can also be weights*normal(1, alpha)
		-alpha = p/(1-p) for bernoulli(p), where p is dropout rate; p->1, alpha -> infinite = DROP that beat
	-Variational infernence is used on the weights parameterized by N(theta_ij, alpha_ij) for each weight w_ij
	-Prior for P(w) is improper log scale uniform
	-Now you can train individual dropouts for each weight/node!

Simplifying Neural Networks by Soft Weight-sharing
http://www.cs.toronto.edu/~hinton/absps/sunspots.pdf
	-People usually use L0, L1, or L2 loss to force weights to 0
	-Add Multivariate guassian complexity term to loss function, use backprop to train mean, variance of guassian
	as well as cluster weights
	-The prior paper to Soft Weight Sharing for NN compression that Erin read

Bayesian Compression for Deep Learning
	-Prune large parts of networks through sparsity priors
	-Use Variational Inference to find P(w|D), elbo strongly depends on prior
	-The prior they used is w ~ N(0, z^2), where z ~ from some distribution (some hierarchal bayes)
		-The distribution they used were Half Cauchy and improper log uniform
	-Perfrom Variational Inference on Q(w, z), sparisty pops up due to resulting "horseshoe"?
	-Need your help @Erin Grant to understand the math, since I'm bad

Meta-Learning Evolutional Neural Networks
	-In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial Neural Network), an automatic
computational framework for the adaptive optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and its parameters are adapted according to the
problem
	-Lowest level of evolutionary heirarchy: weights
		-Basic eovlution algorithm you learn in 188
	-Second lowest level of evolutionary hierarchy: activation function, architecture
		-Constructive Desturction Aglorithms
		-Connections between nodes, represented as 1 or 0
		-Perform your evoluation algorithm for node 1, node 2, ...node n
	-High level ofevolutionary hierarchy: Type of gradient update used (backpropr, or etc)
		-Evolutionary algoriothm

Generative Adversarial Networks
	-Potential Weight Sharing idea between generator and discrminator
	-Generator G maps latent space z to fake image x'
		-Maximize D(G(z)) (Maximize probably discriminator says fake image is real)
	-Discriminator D maps G(z) = x' or x (ground truth) to real or not real
		-Maximize E_p(x)[D(x)] + E_p(z)[1-D(x')]
			-p(z) = p(x') btw
			-Maximize Probability x is real and minimize probability x' is real
	-Algorithm trains Discriminator for k iterations then Generator for 1 iteration
		-Repeat
		-Based off of minimax dual descent

Variational Continual Learning
	-Insipiration behind this paper is that neural networks forget a lot of prior stuff after learning
	from a bunch of tasks
	-You have T Datasets
		-p(theta | D1, D2 .. DT) = (1/Z) p(theta| D1, D2 .. D_(T-1)* p(D_t|theta)
		-Recursion!
	-Where does Variational Inference come in?
		-You have prior q_0(theta)
		-Recrusively update q_t(theta) through this:
			-q_t(theta) = argmin KL( q(theta) || (1/Z)q_(t-1)(theta)*p(D_t|theta))
	-Introduces Coreset C_t, to contain previous information from previous t Datasets
		-Update C_t+1 from C_t and D_t+1 (randomly choose k points from D_t+1 or using algo to do that)
	-VCL Coreset Algorithm
		-Essentially adds coreset to the variational recursion above
		-A bit mathy, requires q*(theta), an approximation of posterior from data not in coreset and
		q(theta), an approximatino of posterior from all data
	-VCL Also used on deep discriminative and generative models
	


Generative Adversarial Networks
	-Potential Weight Sharing idea between generator and discrminator
		-Need to solidify idea first
	-Generator G maps latent space z to fake image x'
		-Maximize D(G(z)) (Maximize probably discriminator says fake image is real)
	-Discriminator D maps G(z) = x' or x (ground truth) to real or not real
		-Maximize E_p(x)[D(x)] + E_p(z)[1-D(x')]
			-p(z) = p(x') btw
			-Maximize Probability x is real and minimize probability x' is real
	-Algorithm trains Discriminator for k iterations then Generator for 1 iteration
		-Repeat
		-Based off of minimax dual descent


Soft Weight Sharing for NN compression
https://arxiv.org/abs/1702.04008
    -Use the soft weight-sharing idea of Nowlan & Hinton (1992) for NN compression.
    -Motivation: Learn a mixture of Gaussians (MoG) prior to encode parameters.
    -Idea: "Compress" weights to K clusters. Works if weights concentrate closely around cluster means.
    -Algorithm: 
        -Take a pretrained network. Initialize a MoG w/ diagonal covariances as a "prior" over the weights.
        -Retrain model with joint log-likelihood (Eq. 7).
        -(Optional: Merge MoG components as necessary using a KL. Easy because component likelihoods are Gaussian!)
        -Quantize by setting each weight to the mean of the component with highest responsibility.
    -Results:
        -Decent for LeNet on MNIST. No baseline for (wide) ResNet.
        -Authors state difficulty with VGG:
            -"In experiences with VGG we were able to prune 93% of the 
             weights without loss of accuracy, however, the quantization step resulted in significant loss of 
             accuracy. We think this is due to the network not having convergened."
             
             
Variational Network Quantization
https://openreview.net/forum?id=ry-TW-WAb
     -Idea: Learn dropout noise levels per weight and prune weights with large dropout noise; 
     -Motivation: Interpret dropout training as variational inference of an approximate weight-posterior
      under a sparsity-inducing prior.
     -Variational posterior approximation and training procedure is similar to Kingma et al. (2015) 
      and Molchanov et al. (2017) with the crucial difference of using a quantizing prior that 
      drives weights towards the target values for quantization.
     -SotA in quantization?

Learning Sparse Neural Networks through L_0 Regularization
https://openreview.net/forum?id=H1Y8hhg0b
    -Motivations:
        -AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization.
        -The log-uniform prior can be seen as a continuous relaxation of the spike-and-slab prior.
    -Idea: 
        -Can smooth the L0 regularized objective in a way that can make it differentiable
        -Allows for efficient gradient based optimization, without needing extra terms to make learning stable
    -Implementation:
        -Optimize a continuous surrogate for L_0 norm via simple modifications of the binary concrete relaxation 
         with additional stretching and hard-sigmoid transformation steps.
        -Allows for conditional computation, since it permits weight values of exactly zero.
        -Also allows structured penalties, i.e. removing either entire convolutional feature maps or entire hidden units. 

MAML
https://arxiv.org/pdf/1703.03400.pdf
    -Really cool paper that actually works! (really impressive results esp sinusoidal results) Chelsea Finn is god
    -Essentially have a model over a distirubtion of tasks P(f_theta)
        -Perform gradietn descent over each task T_i, to obtain theta_i'
        -Update theta with the averaged gradients of loss function for each theta_i'
        -Visually, think of each theta_i' as each task's way of pulling theta in that direction
        -Having the theta to go to the middle of all the "pulling" for all the theta_i's will theoretically yield quickest learning

Meta-learning Shared Hierarchies
https://openreview.net/pdf?id=SyX0IeWAW
    -phi - shared parameters between tasks
	- K subpolicies in phi, phi_i
    -theta - task specific parameters for a single task
    -P_M, distribution of tasks
    -For each task, for each task, there is a master policy theta (action space (1, 2...,K) that chooses among the K subpolicies
    -Master policy chooses phi_i every N timesteps, phi_i is executed in between
    -How to train?
        -Sample task
        -Reset theta
        -Warmup Period
            -train theta
        -Joint Period
            -train theta and phi
        -Repeat forever

Weight Uncertainty in Neural Networks
https://arxiv.org/pdf/1505.05424.pdf
    -Uses variational inference to find a posterior distribution of weights given data in a neural network 
    -Uses monte carlo sampling to train ELBO to find optimal u, sigma for weight distribution

Gradient-based Hyperparameter Optimization through Reversible Learning
https://arxiv.org/abs/1502.03492
    -Most papers that perform gradient based hyperparmaeter optimization stores all the weights w1, ... w_t to find
    gradient for hyperparameters at time step 1
    -This paper finds a way via momentum descent to find gradients w.r.t hyperparemters in O(T) time with constant memory proportional
    to number of weights, not W*T. 
    -Lots of cool applications in the paper (you can treat initialized weights as a hyperparameter! (used later in MAML))
    -CON: to improve hyperparameters, you gotta do a bit of training than go from time T to time 1 to obtain gradient for hyperparameters 
    at that timestep
    -Weightsharing? Between interations of meta improvements?

Hyperparameter optimization with approximate gradient
https://arxiv.org/abs/1602.02355
    -Pretty sure DART's main inspiration was off this paper, the algorithms are very very similar
    -Same idea where you hyperparameter and parameter updating are done in a bi-level optimization like in DART

Dense-CNN
https://arxiv.org/abs/1608.06993
-Concatenate previous layer to calculate new layer
-not really weight sharing, but feature map sharing

Winning Ticket Hypothesis
https://arxiv.org/abs/1803.03635
    - Algorithm:
        -Initialize Network with weights, w0
        -Loop:
            -Train till convergence
            -Prune smallest weights
            -Take the pruned network and reinitialize the remaining weights to w0
    - Larger networks have a larger combination of "winning tickets" (subnetworks that have a good combination during 
    initialization w0 that result in high accuracy), hence the idea of pruning
    -When randomly reinitialized or rearranged, winning tickets perform far worse than the original network, meaning neither structure nor
    initialization alone is responsible for a winning ticket’s success.
    - how do they parameterize the pruned network? do they zero out the weights?
    -A.1: iterative pruning works!! Can we frame this as a bilevel optimization problem?

On the importance of single directions for generalization
https://arxiv.org/abs/1803.06959
    -Selectively ablated neurons may harm classification performance.
    -Can obtain similar results by replacing selectivity by mutual information.

Understanding Individual Neuron Importance Using Information Theory
https://arxiv.org/abs/1804.06679
    -Pruning techniques based on importance measures for individual neurons should 
        1) be applied layer-wise rather than on the NN as a whole
        2) potentially benefit by using different importance measures in different layers
    -Retraining after pruning can be replaced by a small surgery step

DART Architecture Search
https://arxiv.org/abs/1806.09055
    -Optimizes based on cell, not entire network (convolutional or recurrent cell like LSTM or GRU)
    -How Their Cell model works
        -Imagine the cell as a dag with nodes
            -2 nodes for input (from previous 2 convolutional layers) and 1 node for output
            -Cell also contains n intermediary nodes, x1, x2, x3, .... xn
            -The edge connecting between two nodes is a mathematical operation (conv, zero, etc)
                -M mathematical operations (could be anything wtih trainable weights)
            -There are LOTS of edges
                -Both input nodes are connected to x1, ... xn
                -All intermediary nodes are connected to output node
                -For all i<j, node x_i connects to x_j
                -Between any two nodes with a valid connection, there are M edges, each of which is a mathematical operation
                    -IMPORTANT: The output of the M edges is a weighted softmaxed sum of all M mathematical operations
                        -Adds another set of weights, alpha_(i,j), to represent how much each of those M mathemtical operations 
                        should be weighted
    -This becomes a cool dual optimization problem:
        -Given a fixed alpha_(i,j), optimize weights over training set
        -Given fixed weights, optimize alpha over validation set
            -Seems like, since this is no longer the setting of few-shot learning, we should just call this batches 1 & 2!
    -The above optimization problem is expensive, hence the DART algorithm:
        -For each interation of training over weights w, update weights over training set, then update alpha over validation set
            -Different than finding optimal w*, then updating alpha to alpha', then finding optimal w**, etc.
        -Another caveat is that they choose the largest alpha as the only connection between nodes after training
            -ie., this is how they go from the relaxed problem to the original setting of a single architecture
            -Q: is this the best way to relax the architecture search problem?

Neural Processes
https://arxiv.org/abs/1807.02033
    - Variational inference (VI) for modelling distributions over functions.
    - Generative model (from http://kasparmartens.rbind.io/post/np/):
        - First, the context points ( x c , y c ) are mapped through a NN h to obtain a latent representation r c . 
        - Then, the vectors r c are aggregated (in practice: averaged) to obtain a single value r (which has the same dimensionality as every r c ).
        - This r is used to parametrise the distribution of z , i.e. p ( z | x 1 : C , y 1 : C ) = N ( μ z ( r ) , σ 2 z ( r ) ) 
        - Finally, to obtain a prediction at a target x ∗ t , we sample z and concatenate this with x ∗ t , and map ( z , x ∗ t ) through a NN g 
          to obtain a sample from the predictive distribution of y ∗ t



--------------------------------------Unrelated Papers--------------------------------------

Interpreting Deep Visual Representations via Network Dissection
https://arxiv.org/abs/1711.05611
-Not really weight sharing

Neural Module Networks
https://arxiv.org/pdf/1511.02799.pdf
-Not really weight sharing going on here, unless we interpret modularization as a form of weight sharing

Stackable NN Modules
https://arxiv.org/abs/1807.08556
-Don't see the weight sharing in this example
