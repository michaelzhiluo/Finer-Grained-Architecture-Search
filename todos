Thursday, August 16th
-think about weight sharing: 
  -how can we setup the meta-learning problem so that we can learn an interesting weight-sharing structure?
  -how can we adapt "soft weight-sharing" so that online pruning / collapsing is possible?
  -how can we take insight from the Lottery Ticket Hypothesis, which accomplishes drastic pruning with little loss in accuracy?
  -how can we take insight from L0 regularization for online pruning?
-more papers!


Wednesday, September 5th:
-start implementing the generalized conv/FF network, with MNIST as a toy dataset
  -what is an efficient way to perform the elementwise multiplication?
  -two versions of the loss function:
    -(a) the DARTS version, with \alpha parameterizing a Bernoulli: 
      (step i)  to update weights w, compute grad_w L(w, x_train, \alpha) 
      (step ii) to update \alpha,    compute grad_\alpha L(w_0 - grad_w L(w, x_train, \alpha), x_train, \alpha)
    -(b) the variational version: define q_\theta (w, \alpha) and maximize the ELBO
  -try (a) first, as it is likely simpler
    -are there any alternatives to these two choices of loss function?
  
-think about the generalized net's relation to:
  -relational networks (e.g., https://arxiv.org/abs/1706.01427)
  -equivariant networks
  
-how can we make the search problem over \alpha_ijk easier?
